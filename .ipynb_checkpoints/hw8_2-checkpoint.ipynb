{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hw8_dataload import LFD_Data2\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 8\n",
    "## Primal vs Dual Problem\n",
    "\n",
    "- The SVM **primal** problem minimizes 0.5w<sup>T</sup>w subject to y<sub>n</sub>(w<sup>T</sup>x<sub>n</sub>+b)&ge;1 for n = 1,2,...,N\n",
    "- Since w is a d-dimensional vector (corresponding to the dimension of x) and we can also vary b, the primal problem involves a quadratic programming problem with **d+1 variables**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernels\n",
    "\n",
    "- Implementing polynimal kernels with a soft-margin SVM using the given data set of handwritten digits from the US Postal Service Zip Code data set with extracted features digit, intensity, and symmetry. \n",
    "- The polynomial kernel K(x<sub>n</sub>, x<sub>m</sub>) = (1+x<sub>n</sub><sup>T</sup>x<sub>m</sub>)<sup>Q</sup>\n",
    "- Training **two** types of binary classifiers:\n",
    "    - one-vs-one (one digit class is +1, another is -1, rest are ignored)\n",
    "    - one-vs-all (one digit class is +1, everything else is -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw8_train = \"datasets/features.train\"\n",
    "hw8_test = \"datasets/features.test\"\n",
    "hw8_C = 0.01\n",
    "hw8_Q = 2\n",
    "hw8_data = LFD_Data2(hw8_train, hw8_test)\n",
    "\n",
    "my_svm = svm.SVC(C = 0.01, kernel = 'poly',degree = 2, coef0 = 1.0, gamma = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-vs-all binary classifier in-sample error: 0.105884\n",
      "1-vs-all binary classifier in-sample error: 0.014401\n",
      "2-vs-all binary classifier in-sample error: 0.100261\n",
      "3-vs-all binary classifier in-sample error: 0.090248\n",
      "4-vs-all binary classifier in-sample error: 0.089425\n",
      "5-vs-all binary classifier in-sample error: 0.076258\n",
      "6-vs-all binary classifier in-sample error: 0.091071\n",
      "7-vs-all binary classifier in-sample error: 0.088465\n",
      "8-vs-all binary classifier in-sample error: 0.074338\n",
      "9-vs-all binary classifier in-sample error: 0.088328\n",
      "Diff in number of sv's between odd and even: 2071\n"
     ]
    }
   ],
   "source": [
    "alphas_odd = np.array([])\n",
    "alphas_even = np.array([])\n",
    "\n",
    "for cur_num in range(10):\n",
    "    #cur_num-vs-all\n",
    "    hw8_data.set_filter([cur_num])\n",
    "    cur_X = hw8_data.get_X(\"train\")\n",
    "    cur_Y = hw8_data.get_Y(\"train\")\n",
    "    my_svm.fit(cur_X, cur_Y)\n",
    "    cur_score = my_svm.score(cur_X, cur_Y)\n",
    "    cur_numalphas = my_svm.n_support_\n",
    "    cur_asum = np.array(cur_numalphas).sum()\n",
    "    print(\"%d-vs-all binary classifier in-sample error: %f\" % (cur_num, (1.0 - cur_score)))\n",
    "    if cur_num % 2 == 0:\n",
    "        alphas_even = np.concatenate((alphas_even, [cur_asum]))\n",
    "    else:\n",
    "        alphas_odd = np.concatenate((alphas_odd, [cur_asum]))\n",
    "    \n",
    "    \n",
    "aodd_sum = np.sum(alphas_odd)\n",
    "aeven_sum = np.sum(alphas_even)\n",
    "a_diff = abs(aodd_sum - aeven_sum)\n",
    "print(\"Diff in number of sv's between odd and even: %d\" % a_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With C = 0.01, Q=2 and with a n-vs-all classifier, it turns out that 0 out of all the evens has the highest in-sample error and 1 out of all the odds has the lowest in-sample error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1561, 2) (1561,) (424, 2) (424,)\n"
     ]
    }
   ],
   "source": [
    "#loading 1-vs-5 data\n",
    "\n",
    "hw8_data.set_filter([1,5])\n",
    "x_1v5_train = hw8_data.get_X(\"train\")\n",
    "y_1v5_train= hw8_data.get_Y(\"train\")\n",
    "x_1v5_test = hw8_data.get_X(\"test\")\n",
    "y_1v5_test= hw8_data.get_Y(\"test\")\n",
    "\n",
    "print(x_1v5_train.shape, y_1v5_train.shape, x_1v5_test.shape, y_1v5_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ For polynomial kernels of degree Q = 2 ~~~\n",
      "C = 0.000100 | E_in = 0.008969, E_out = 0.016509, num_sv = 236\n",
      "C = 0.001000 | E_in = 0.004484, E_out = 0.016509, num_sv = 76\n",
      "C = 0.010000 | E_in = 0.004484, E_out = 0.018868, num_sv = 34\n",
      "C = 0.100000 | E_in = 0.004484, E_out = 0.018868, num_sv = 24\n",
      "C = 1.000000 | E_in = 0.003203, E_out = 0.018868, num_sv = 24\n",
      "\n",
      "~~~ For polynomial kernels of degree Q = 5 ~~~\n",
      "C = 0.000100 | E_in = 0.004484, E_out = 0.018868, num_sv = 26\n",
      "C = 0.001000 | E_in = 0.004484, E_out = 0.021226, num_sv = 25\n",
      "C = 0.010000 | E_in = 0.003844, E_out = 0.021226, num_sv = 23\n",
      "C = 0.100000 | E_in = 0.003203, E_out = 0.018868, num_sv = 25\n",
      "C = 1.000000 | E_in = 0.003203, E_out = 0.021226, num_sv = 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pk_Q = [2,5]\n",
    "pk_C = [pow(10, -x) for x in reversed(range(5))]\n",
    "\n",
    "for Q in pk_Q:\n",
    "    my_svm.degree = Q\n",
    "    print(\"~~~ For polynomial kernels of degree Q = %d ~~~\" % Q)\n",
    "    for C in pk_C:\n",
    "        my_svm.C = C\n",
    "        my_svm.fit(x_1v5_train, y_1v5_train)\n",
    "        cur_ein = 1.0 - my_svm.score(x_1v5_train, y_1v5_train)\n",
    "        cur_eout = 1.0 - my_svm.score(x_1v5_test, y_1v5_test)\n",
    "        cur_numalphas = my_svm.n_support_\n",
    "        cur_asum = np.array(cur_numalphas).sum()\n",
    "        print(\"C = %f | E_in = %f, E_out = %f, num_sv = %d\" % (C, cur_ein, cur_eout, cur_asum))\n",
    "    print(\"\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous output, we can see that the maximum C (C being the upper bound on &alpha;<sub>i</sub>'s, which are \"weights\" for support vector contributions) achieves the lowest E<sub>in</sub>. Furthermore, the number support vectors for Q = 5 (Q being the degree of the polynomial kernel) is lower for C = 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "With Q = 2 and C &isin; {0.0001, 0.001, 0.01, 0.1, 1} using a polynomial kernel on the 1 vs 5 classifier (as used above), we will now use 10-fold cross validation, with the cross-validation erro E<sub>CV</sub> = (1/N) &sum;(n=1;N){e<sub>n</sub>}.e<sub>n</sub> is the validation error for a fold and if g<sup>-</sup><sub>n</sub> is the hypothesis learned on that fold, e<sub>n</sub> = e(g<sup>-</sup><sub>n</sub>(x<sub>n</sub>), y<sub>n</sub>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.000100 is selected most often with average E_cv 0.004483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv_Q = 2\n",
    "cv_C = [pow(10, -x) for x in reversed(range(5))]\n",
    "cv_runs = 100 #number of runs\n",
    "cv_splits = 10 #number of splits\n",
    "\n",
    "#note that k-fold validation is not in sklearn 0.17\n",
    "\n",
    "\n",
    "\n",
    "e_cvs = np.ndarray((0, len(cv_C)))\n",
    "cv_winner = np.zeros(len(cv_C)) #record of the winning C each run\n",
    "#iterate over runs\n",
    "for cur_run in range(cv_runs):\n",
    "    #iterate over possible c values\n",
    "    cur_ecvs = np.array([])\n",
    "    cv_kf = KFold(n_splits=cv_splits, shuffle=False)\n",
    "    for C in cv_C:\n",
    "        my_svm.C = C\n",
    "        e_vals = np.array([]) #array of validation errors\n",
    "        #iterate over each fold\n",
    "        for train_idx, test_idx in cv_kf.split(x_1v5_train):\n",
    "            cv_xtrain, cv_xtest = x_1v5_train[train_idx], x_1v5_train[test_idx]\n",
    "            cv_ytrain, cv_ytest = y_1v5_train[train_idx], y_1v5_train[test_idx]\n",
    "            my_svm.fit(cv_xtrain, cv_ytrain)\n",
    "            cur_err = 1.0 - my_svm.score(cv_xtest, cv_ytest)\n",
    "            e_vals = np.concatenate((e_vals, [cur_err]))\n",
    "        cur_ecv = np.average(e_vals) #current cv error\n",
    "        cur_ecvs = np.concatenate((cur_ecvs, [cur_ecv]))\n",
    "    win_idx = np.argmin(cur_ecvs) #index of the winning C\n",
    "    #mark winner in our records\n",
    "    cv_winner[win_idx] = cv_winner[win_idx] + 1\n",
    "    #add cv errors to our records\n",
    "    e_cvs = np.vstack((e_cvs, cur_ecvs))\n",
    "\n",
    "#find average e_cvs for each C\n",
    "ecv_avg = np.average(e_cvs, axis=0)\n",
    "overall_winner = np.argmax(cv_winner)\n",
    "ecv_win = ecv_avg[overall_winner]\n",
    "\n",
    "print(\"C = %f is selected most often with average E_cv %f\" % (cv_C[overall_winner], ecv_win))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Kernel\n",
    "\n",
    "Now we are going to use the radial basis function (RBF) kernel K(x<sub>n</sub>, x<sub>m</sub>) = exp(-&Vert;x<sub>n</sub> - x<sub>m</sub>&Vert;<sup>2</sup>) with a soft-margin SVM as a 1 vs 5 classifier.\n",
    "\n",
    "We will test our SVM on various values of C &isin; {0.01, 1, 100, 10<sup>4</sup>, 10<sup>6</sup>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.010000 | E_in = 0.003844, E_out = 0.023585\n",
      "C = 1.000000 | E_in = 0.004484, E_out = 0.021226\n",
      "C = 100.000000 | E_in = 0.003203, E_out = 0.018868\n",
      "C = 10000.000000 | E_in = 0.002562, E_out = 0.023585\n",
      "C = 1000000.000000 | E_in = 0.000641, E_out = 0.023585\n"
     ]
    }
   ],
   "source": [
    "rbf_C = [pow(10,x) for x in range(-2,7,2)]\n",
    "\n",
    "my_svm.kernel = 'rbf'\n",
    "my_svm.gamma = 1\n",
    "\n",
    "for C in rbf_C:\n",
    "    my_svm.C = C\n",
    "    my_svm.fit(x_1v5_train, y_1v5_train)\n",
    "    cur_ein = 1.0 - my_svm.score(x_1v5_train, y_1v5_train)\n",
    "    cur_eout = 1.0 - my_svm.score(x_1v5_test, y_1v5_test)\n",
    "    print(\"C = %f | E_in = %f, E_out = %f\" % (C, cur_ein, cur_eout))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that C = 10<sup>6</sup> has the lowest E<sub>in</sub> while C = 100 has the lowest E<sub>out</sub>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
