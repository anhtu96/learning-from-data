{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from hw6_nlt import LinRegNLT2\n",
    "from hw6_dataload import LFD_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6\n",
    "## Overfitting and Deterministic Noise\n",
    "Given hypothesis set &Hscr; of target function f and &Hscr;' &subset; &Hscr;, using &Hscr;' in general will lead to **increased deterministic noise** since we will have less hypotheses available at our disposal to deal with a higher-order determinstic function (and moreso, deterministic noise can only increase by using less than the available number of hypotheses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization with Weight Decay\n",
    "\n",
    "**Given :** two-dim input x = (x1, x2) so that &Xscr; = &reals;<sup>2</sup> with label &Yscr; = {-1,1}. \n",
    "\n",
    "**Want :** Linear Regression with a non-linear transformation for classification given by:\n",
    "\n",
    "&phi;(x1, x2) = (1, x1, x2, x1<sup>2</sup>, x2<sup>2</sup>, x1x2, |x1-x2|, |x1+x2|)\n",
    "\n",
    "\n",
    "Classification error is defined as the fraction of misclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression without Weight Decay:\n",
      "E_in: 0.028571, E_out: 0.084000\n",
      "Linear Regression with Weight Decay (k = -3):\n",
      "E_in: 0.028571, E_out: 0.080000\n",
      "Linear Regression with Weight Decay (k = -2):\n",
      "E_in: 0.028571, E_out: 0.084000\n",
      "Linear Regression with Weight Decay (k = -1):\n",
      "E_in: 0.028571, E_out: 0.056000\n",
      "Linear Regression with Weight Decay (k = 0):\n",
      "E_in: 0.000000, E_out: 0.092000\n",
      "Linear Regression with Weight Decay (k = 1):\n",
      "E_in: 0.057143, E_out: 0.124000\n",
      "Linear Regression with Weight Decay (k = 2):\n",
      "E_in: 0.200000, E_out: 0.228000\n",
      "Linear Regression with Weight Decay (k = 3):\n",
      "E_in: 0.371429, E_out: 0.436000\n"
     ]
    }
   ],
   "source": [
    "rwd_train = \"hw6_train.dta\"\n",
    "rwd_test = \"hw6_test.dta\"\n",
    "l_reg = math.pow(10.0, -3) #lambda regularization term\n",
    "\n",
    "# load data from external files and init\n",
    "rwd_data = LFD_Data(rwd_train, rwd_test)\n",
    "rwd_algo = LinRegNLT2(rwd_data.dim, 7, l_reg)\n",
    "\n",
    "def rwd_print_error(algo,data):\n",
    "    ein = algo.calc_error(data.train_X, data.train_Y)\n",
    "    eout = algo.calc_error(data.test_X, data.test_Y)\n",
    "    print(\"E_in: %f, E_out: %f\" % (ein, eout))\n",
    "    \n",
    "\n",
    "#train without regularization\n",
    "rwd_algo.train(rwd_data.train_X, rwd_data.train_Y)\n",
    "print(\"Linear Regression without Weight Decay:\")\n",
    "rwd_print_error(rwd_algo, rwd_data)\n",
    "\n",
    "\n",
    "rwd_k = np.arange(-3, 4)\n",
    "\n",
    "for k in rwd_k:\n",
    "    cur_lam = math.pow(10.0, k)\n",
    "    rwd_algo.set_lambda(cur_lam)\n",
    "    rwd_algo.train_reg(rwd_data.train_X, rwd_data.train_Y)\n",
    "    print(\"Linear Regression with Weight Decay (k = %d):\" % k)\n",
    "    rwd_print_error(rwd_algo, rwd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization for Polynomials\n",
    "\n",
    "**Given :** Transform from a linear model to a space &Zscr; given by &Phi;: &Xscr; &rarr; &Zscr; where z &isin; &Zscr; is a vector of Legendre polynomials (1, L<sub>1</sub>(x), L<sub>2</sub>(x),...,L<sub>Q</sub>(x)) and the hypthosis set &Hscr;<sub>Q</sub> is given by:\n",
    "\n",
    "&Hscr;<sub>Q</sub> = { h | h(x) = w<sup>T</sup>z= = &sum;(q=0;Q){&wscr;<sub>q</sub>L<sub>q</sub>(x)}}, where L<sub>0</sub>(x) = 1\n",
    "\n",
    "\n",
    "\n",
    "Given the constrained hypothesis set:\n",
    "\n",
    "&Hscr;(Q,C,Q<sub>o</sub>) = { h | h(x) = w<sup>T</sup>z &isin; &Hscr;<sub>Q</sub>; &wscr;<sub>q</sub> = C for q &ge; Q<sub>o</sub>}\n",
    "\n",
    "\n",
    "we see that if C = 0, it doesn't have an polynomials of degree &ge; Q<sub>o</sub> and thus the largest degree polynomial in this polynomial set is of degree Q<sub>o</sub> - 1. Thus:\n",
    "\n",
    "\n",
    "&Hscr;(10,0,3) &Intersection; &Hscr;(10,0,4) = &Hscr;<sub>2</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "**Given :** fully connected neural network with L = 2; d<sup>(0)</sup> = 5, d<sup>(1)</sup> = 3, d<sup>(2)</sup> = 1 only counting products of the form w<sub>ij</sub><sup>(l)</sup>x<sub>i</sub><sup>(l-1)</sup>, w<sub>ij</sub><sup>(l)</sup>&delta;<sub>j</sub><sup>(l)</sup>, and x<sub>i</sub><sup>(l-1)</sup>&delta;<sub>j</sub><sup>(l)</sup> as operations. The &delta;<sub>j</sub><sup>(l)</sup> are partial derivatives &PartialD;e(w)/&PartialD;s<sub>j</sub><sup>(l)</sup> where e(w) is the error function.\n",
    "\n",
    "\n",
    "Furthermore, s<sub>j</sub>'s are given as so:\n",
    "\n",
    "s<sub>j</sub><sup>(l)</sup> = &sum;(i=0;d<sup>(l-1)</sup>){w<sub>ij</sub><sup>(l)</sup>x<sub>i</sub><sup>(l-1)</sup>}. (1)\n",
    "\n",
    "\n",
    "\n",
    "&delta;<sub>j</sub><sup>(l)</sup> other than l = L can be calculated like so:\n",
    "\n",
    "&delta;<sub>i</sub><sup>(l-1)</sup> = (1-(x<sub>i</sub><sup>(l-1)</sup>)<sup>2</sup>)&sum;(j=1;d<sup>(l)</sup>){w<sub>ij</sub><sup>(l)</sup>&delta;<sub>j</sub><sup>(l)</sup>} (2)\n",
    "\n",
    "\n",
    "After backpropagation, weights are updated with the following equation:\n",
    "\n",
    "\n",
    "w<sub>ij</sub><sup>(l)</sup> = w<sub>ij</sub><sup>(l)</sup> - &eta;x<sub>i</sub><sup>(l-1)</sup>&delta;<sub>j</sub><sup>(l)</sup> (3)\n",
    "\n",
    "\n",
    "### Calculating operations needed for backpropagation\n",
    "\n",
    "- Calculating the xi's requires the use of ** w<sub>i</sub><sup>(l)</sup>x<sub>i</sub><sup>(l-1)</sup>** terms since x<sub>i</sub> = &theta;(s<sub>i</sub>) and equation (1) for the si's are given above. The input layer has 6 nodes x0 = 1, x1, x2, x3, x4, x5 going into 3 nodes in layer 1, making 18 operations. Layer 1 has 4 nodes (including the additional i = 0 constant node) going into 1 node in layer 2, making 4 operations and 18 + 4 = **22** operations to calculate all the xi's.\n",
    "- For backpropagation proper, we are not considering the operations necessary to calculate &delta;<sub>1</sub><sup>L</sup> (not listed above). To calculate the &delta;s for l < 2, we consider equation (2) given above with the **w<sub>ij</sub><sup>(l)</sup>&delta;<sub>j</sub><sup>(l)</sup>**). For each l, x<sub>0</sub> = 1 and thus equation (2)'s coefficient will be 0 and thus &delta;<sub>0</sub>'s will not factor into our operation count. Thus for the &delta;<sup>(1)</sup>'s, we have 3 operations, 1 for each &delta; since there is only one term in each right-hand sum w<sub>ij</sub>(2)&delta;<sub>j</sub>(2) since d<sup>2</sup> =  1. Since there's no s<sub>i</sub> signals for the input layer l = 0, we do not calculate any &delta;<sub>i</sub>'s for them and thus l = 0 contributes 0 operations. Thus for backpropogation we have 3 + 0 = **3** operations.\n",
    "- Updating the weights requires use of **x<sub>i</sub><sup>(l-1)</sup>&delta;<sub>j</sub><sup>(l)</sup>** products. Everywhere we used a weight, we need to apply equation (3) and each application involves one product term each. As we found in the feedforward calculation of the xi's, there are 22 weights and thus **22** operations, 6 &times; 3 = 18 going from l=0 to l=1, 4 &times; 1 = 4 going from l=1 to l=2. \n",
    "- Thus overall, there are 22 + 3 + 22 = **47** operations used.\n",
    "\n",
    "\n",
    "**Given :** Neural network with 10 input units (counting x0s), 36 hidden units (counting x0s), and 1 output unit, with the hidden units arrangeable in any number of layers l = 1,...,L-1 and each layer fully connected to the layer above it.\n",
    "\n",
    "- The **minumum** possible number of weights this network could have would be if each node in the hidden layers had its own layer. Then, we could have 10 weights for the input layer feeding into the first hidden layer and 36 weights, 1 for each hidden layer giving a total of **46** weights.\n",
    "- Transitioning from one layer to the next requires &rho; = d<sup>(l-1)</sup> &times; (d<sup>(l)</sup> - 1) weights (since we don't feed into the x0 terms) and thus we want to somehow maximize this quantity. This occurs when the d<sup>(l-1)</sup> = d<sup>(l)</sup> - 1 since the product should be the closest to forming a square as possible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With hidden layer dimensions:\n",
      "[22, 14]\n",
      "We require 510 weights.\n"
     ]
    }
   ],
   "source": [
    "hidden_layers = [22,14]\n",
    "\n",
    "def number_of_weights(d_list):\n",
    "    prev_d = 10 #number of nodes in input list\n",
    "    num_w = 0 #number of weights\n",
    "    for d in d_list:\n",
    "        num_w = num_w + ((d- 1)  * prev_d)\n",
    "        prev_d = d\n",
    "    return num_w + d_list[-1]\n",
    "\n",
    "print(\"With hidden layer dimensions:\")\n",
    "print(hidden_layers)\n",
    "print(\"We require %d weights.\" % number_of_weights(hidden_layers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above reasoning as a jumping off point (and some fiddling around with values), the **maximum** number of weights needed by the given neural network is **510**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
