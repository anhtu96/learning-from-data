{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(u, v):\n",
    "    return (u*np.exp(v) - 2*v*np.exp(-u))**2\n",
    "\n",
    "def dEu(u, v):\n",
    "    return 2*(np.exp(v) + 2*v*np.exp(-u))*(u*np.exp(v) - 2*v*np.exp(-u))\n",
    "\n",
    "def dEv(u, v):\n",
    "    return 2*(u*np.exp(v) - 2*np.exp(-u))*(u*np.exp(v) - 2*v*np.exp(-u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of loops: 10, error: 1.2086833944220747e-15, u=0.045, v=0.024\n"
     ]
    }
   ],
   "source": [
    "n_iter = 0\n",
    "u, v = 1, 1\n",
    "eta = 0.1\n",
    "while error(u, v) > 10**-14:\n",
    "    n_iter +=1\n",
    "    \n",
    "    # When doing GD for each variable, keep other vars' values\n",
    "    u_tmp, v_tmp = u, v\n",
    "    u -= eta*dEu(u_tmp, v_tmp)\n",
    "    v -= eta*dEv(u_tmp, v_tmp)\n",
    "print('Num of loops: {}, error: {}, u={:.3f}, v={:.3f}'.format(n_iter, error(u, v), u, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error after 15 iters: 0.13981379199615315\n"
     ]
    }
   ],
   "source": [
    "u, v = 1, 1\n",
    "eta = 0.1\n",
    "for i in range(15):\n",
    "    # coordinate GD\n",
    "    u -= eta*dEu(u, v)\n",
    "    v -= eta*dEv(u, v)\n",
    "print('Error after 15 iters: {}'.format(error(u, v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_line():\n",
    "    line_points = np.random.uniform(-1, 1, [2, 2])\n",
    "    line_coef = (line_points[1,1] - line_points[0, 1])/(line_points[1,0] - line_points[0, 0])\n",
    "    line_intercept = line_points[0,1] - line_coef*line_points[0,0]\n",
    "    return line_coef, line_intercept\n",
    "\n",
    "def generate_data(N, line_coef, line_intercept):\n",
    "    X = np.random.uniform(-1, 1, [N, 2])\n",
    "    y = X[:, 1] - X[:, 0] * line_coef - line_intercept >= 0\n",
    "    y = np.where(y, 1, -1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X, w):\n",
    "    if len(X.shape) == 2:\n",
    "        return 1 / (1 + np.exp(X.dot(w.T)))\n",
    "    return 1 / (1 + np.exp(X.reshape(1, -1).dot(w.T)))\n",
    "\n",
    "def error(X, y, w):\n",
    "    N = len(y)\n",
    "    err_sum = 0\n",
    "    for i in range(N):\n",
    "        err_sum += np.log(1 + np.exp(-y[i] * w.dot(X[i])))\n",
    "    return err_sum[0] / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average epochs: 343.72\n",
      "Average Eout: 0.10138423000569995\n"
     ]
    }
   ],
   "source": [
    "n_loops = 100\n",
    "e_out = []\n",
    "epochs = []\n",
    "for loop in range(n_loops):\n",
    "    line_coef, line_intercept = generate_line()\n",
    "    X_train, y_train = generate_data(100, line_coef, line_intercept)\n",
    "    X_train = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)\n",
    "    w = np.zeros((1, 3))\n",
    "    eta = .01\n",
    "    diff = 100\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        \n",
    "        w_prev = w.copy()\n",
    "        for i in np.random.permutation(100):\n",
    "            gradE = -(y_train[i]*X_train[i]) / (1 + np.exp(y_train[i]*(w.dot(X_train[i]))))\n",
    "            w -= eta * gradE\n",
    "        epoch += 1\n",
    "        if np.linalg.norm(w - w_prev) < .01:\n",
    "            epochs.append(epoch)\n",
    "            break\n",
    "    \n",
    "    X_test, y_test = generate_data(100, line_coef, line_intercept)\n",
    "    X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n",
    "    e_out.append(error(X_test, y_test, w))\n",
    "\n",
    "print('Average epochs: {}'.format(np.mean(epochs)))\n",
    "print('Average Eout: {}'.format(np.mean(e_out)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
